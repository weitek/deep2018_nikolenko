# Конспект книги Николенко "Глубокое обучение"

[Николенко С.И. на wikipedia](https://ru.wikipedia.org/wiki/Николенко,_Сергей_Игоревич "Николенко Сергей Игоревич")

## Часть I. Как обучать нейронные сети

### Глава 1. От биологии к информатике, *или We need to go deeper*

> Первая глава — вводная. В ней мы:

- узнаем, что в машинном обучении недавно произошла революция и революция эта все еще длится;
- вспомним историю искусственного интеллекта как науки;
- познакомимся с разными задачами машинного обучения, узнаем, как они связаны и чем отличаются друг от друга;
- выясним, почему человеческий мозг может служить образцом для подражания при создании искусственного интеллекта;
- придем к выводу, что нейробиология пока не слишком точная наука;
- закончим кратким обзором самых ярких примеров применения современных нейронных сетей и областей, где им еще есть чему поучиться.

#### 1.1. Революция обучения глубоких сетей

> В 2005–2006 годах группы исследователей под руководством
> Джеффри Хинтона (Geoffrey Hinton) в университете Торонто и Йошуа Бенджи (Yoshua Bengio)
> в университете Монреаля научились обучать *глубокие нейронные сети*.

> Решение, предложенное группой Хинтона в середине 2000-х годов, пришло
> в виде *предобучения без учителя*, когда сеть сначала обучается на большом наборе
> данных без разметки, а потом уже дообучается на размеченных данных, используя
> это приближение. Например, если мы хотим распознавать человеческие лица, то
> давайте сначала пообучаем нейронную сеть на фотографиях с людьми вообще, без
> разметки (таких фотографий можно легко набрать сколь угодно много), а уже
> потом, когда сеть «насмотрится» на неразмеченные фотографии, дообучим ее на
> имеющемся размеченном наборе данных. Оказалось, что при этом конечный результат
> становится намного лучше, а хорошие и интересные признаки сеть начинает
> выделять еще на этапе предобучения без учителя.

#### 1.2. Искусственный интеллект и машинное обучение

> во второй половине 1950-х и начале 1960-х годов, появились и
> самообучающиеся машины, в частности перцептрон Розенблатта

> 1970-е годы стали временем расцвета *систем, основанных на знаниях*
> (knowledge-based systems), которые до сих пор являются важной частью науки
> об *экспертных системах*. Суть здесь состоит в том, чтобы накопить достаточно
> большой набор правил и знаний о предметной области, а затем делать выводы.
> Одним из самых ярких примеров таких проектов стала система MYCIN, посвященная
> идентификации бактерий, вызывающих серьезные инфекции, а затем рекомендующая
> антибиотики [65, 494]. В ней было около 600 правил, и ее результаты (она
> выдавала подходящий метод лечения в 69% случаев) были не хуже, чем у опытных
> врачей, и существенно лучше, чем у начинающих. Более того, такие системы
> могли объяснить, как именно они пришли к тому или иному решению, и оценить
> свою уверенность в этой гипотезе. Позднее они стали прообразами графических
> вероятностных моделей.

> Затем исследователи снова вспомнили о нейронных сетях: к началу 1980-х
> годов был разработан алгоритм обратного распространения ошибки (его мы
> подробно обсудим в главе 2), что открыло дорогу самым разнообразным архитектурам.
> Одним из ключевых понятий 1980-х годов был *коннекционизм* (connectionism):
> пришедшая из когнитивных наук идея о том, что нужно не пытаться задавать
> аксиоматические формальные системы для последующих рассуждений в них, а
> строить большие ансамбли параллельно работающих нейронов, которые, подобно
> человеческому мозгу, как-нибудь сами разберутся, что им делать. К концу
> восьмидесятых уже появилась бо􀀀льшая часть основных архитектур, о которых мы будем
> говорить в этой книге: сверточные сети, автокодировщики, рекуррентные сети...

> Поэтому вторая волна увлечения
> искусственным интеллектом закончилась в начале девяностых, когда многие
> компании не смогли оправдать завышенных ожиданий и лопнули.

> В 1990-е годы основной акцент сместился на машинное обучение и поиск
> закономерностей в данных, причем нейронные сети, как мы уже упоминали выше, не
> считались особенно перспективными. Зато самих данных, особенно с развитием
> Интернета, становилось все больше, компьютеры становились все быстрее. В
> итоге в середине 2000-х годов очередная новая идея наконец-то сработала, к ней
> быстро подтянулись другие

#### 1.3. Немного о словах: каким бывает машинное обучение

> что такое, собственно, *машинное обучение* (machine learning).
> Интуитивно понятно, что «обучение» — это когда некая модель каким-то образом
> «обучается», а потом начинает выдавать результаты, то есть, скорее всего, что-то
> предсказывать. Можно даже дать очень общее определение «обучаемости»,
> примерно такое, какое дает Томас Митчелл в классической книге «Машинное
> обучение» [368]: «Компьютерная программа *обучается* по мере накопления опыта
> относительно некоторого класса задач T и целевой функции P, если качество решения
> этих задач (относительно P) улучшается с получением нового опыта».

> Хотя это определение звучит чрезвычайно обобщенно и абстрактно, оно на
> самом деле позволяет прояснить некоторые важные моменты. Например,
> центральное место в нем занимают не данные (хотя они тоже есть), а целевая функция.
> Когда вы начинаете решать любую практическую задачу, крайне важно еще «на
> берегу» определить целевую функцию, договориться о том, как вы будете
> оценивать результаты. Выбор целевой функции полностью определяет всю дальнейшую
> работу, и даже в похожих задачах разные целевые функции могут привести к
> совершенно разным моделям. Например, было бы здорово «научить компьютер
> читать», но сначала нужно определить, что это, собственно, значит. Уметь правильно
> отвечать на вопросы по «прочитанному» тексту? Сделать синтаксический разбор
> предложения? Показать самые релевантные данному тексту статьи «Википедии»?
> Разные ответы приводят к разным моделям и направлениям исследований.

> Два основных класса задач машинного обучения — это задачи
> *обучения с учителем* (supervised learning) и
> *обучения без учителя* (unsupervised learning).

> При обучении с учителем на вход подается набор тренировочных примеров,
> который обычно называют *обучающим* или *тренировочным набором данных* (training
> set или training sample — тренировочная выборка), и задача состоит в том, чтобы
> продолжить уже известные ответы на новый опыт, выраженный обычно в виде
> *тестового набора* данных (test set, test sample). Основное предположение здесь в том,
> что данные, доступные для обучения, будут чем-то *похожи* на данные, на которых
> потом придется применять обученную модель, иначе никакое обобщение будет
> невозможно. Для «чтения» текста пример обучения с учителем — это обучение
> модели, которая строит деревья синтаксического разбора предложений (какие слова
> от каких зависят) по набору деревьев, построенных людьми для конкретных
> предложений. Предположение здесь в том, что деревья разбора строятся по одним и тем
> же законам, и модель, обученную на некотором наборе деревьев разбора, можно
> будет применить и к новым предложениям, не входящим в обучающий набор. Если
> это предположение нарушится, модель работать не будет. Например, если
> лингвисты размечали предложения на английском языке, а потом применили обученную
> модель к немецкому, где буквы примерно те же, но синтаксис совершенно другой,
> ожидать от модели разумного поведения не стоит.

> Задачи обучения с учителем обычно делятся на задачи *классификации* и
> *регрессии*. В задаче классификации нужно поданный на вход объект определить в один из
> (обычно конечного числа) классов, например, разделить фотографии животных на
> кошек, собак, лошадей и «все остальное»; или по фотографии человеческого лица
> понять, кто из ваших друзей в социальной сети на ней изображен. Если
> продолжать пример с языком, то типичная задача классификации — это разметка слов
> по частям речи. А в задаче регрессии нужно предсказать значение некоей
> функции, у которой обычно может быть бесконечно много разных значений. Например,
> по росту человека предсказать его вес, сделать прогноз завтрашней погоды,
> предсказать цену акции или, скажем, выделить на фотографии прямоугольник, в
> котором находится человеческое лицо — сделать это необходимо, чтобы затем эти
> прямоугольники подать на вход упомянутому выше классификатору.

> Деление на регрессию и классификацию, конечно, очень условно, можно
> легко придумать «промежуточные» примеры (тот же разбор предложения — к какому
> классу отнести задачу «построить дерево»?). Но обычно ясно, какую задачу мы
> решаем, и это деление имеет содержательный смысл: меняются целевые функции
> и, как следствие, процесс обучения. А есть и откровенно иные виды задач, не
> укладывающиеся в эту несложную классификацию. Например, в поисковых и
> рекомендательных системах часто встречается задача *обучения ранжирования* (learning to
> rank). Она ставится так: по имеющимся данным (в поисковой системе это будут
> тексты документов и прошлое поведение пользователей) отранжировать,
> расставить имеющиеся объекты в порядке убывания целевой функции (в поисковой
> системе она называется *релевантностью*: насколько данный документ подходит,
> что-бы выдать его в ответ на данный запрос). Эта задача чем-то похожа на задачу
> регрессии — нам так или иначе нужно предсказывать непрерывную целевую
> функцию, ту самую релевантность. Но при этом значений самой функции в данных
> совсем нет, да они нам и не важны. Имеют значение только результаты сравнения
> этой функции на разных объектах (какой документ будет выше другого в
> поисковой выдаче). Это приводит к ряду интересных и специфических методов обучения.

> Типичнейший пример задачи обучения без
> учителя — это *кластеризация* (clustering): нужно разбить данные на заранее
> неизвестные классы по некоторой мере похожести так, чтобы точки, отнесенные к
> одному и тому же кластеру, были как можно ближе друг к другу, как можно более
> похожи, а точки из разных кластеров были бы как можно дальше друг от друга, как
> можно менее похожи. Например, решив задачу кластеризации, можно выделить
> семейства генов из последовательностей нуклеотидов, или кластеризовать
> пользователей вашего веб-сайта и персонализовать его под каждый кластер, или
> сегментировать медицинский снимок, чтобы легко было понять, где же там опухоль.

> Еще одна часто встречающаяся задача обучения без учителя — это
> *снижение размерности* (dimensionality reduction), когда входные данные имеют большую
> размерность (например, если у вас на входе разбитый на слова текст, размерность
> будет исчисляться десятками тысяч, а если фотографии — миллионами), а задача
> состоит в том, чтобы построить представление данных меньшей размерности,
> которое тем не менее будет достаточно полно отражать исходные данные. То есть,
> например, по представлению меньшей размерности можно будет достаточно
> успешно реконструировать исходные точки большой размерности. Это можно
> рассматривать как частный случай общей задачи *выделения признаков* (feature extraction),
> и мы будем подробно говорить об этом на протяжении всей книги, а особенно в
> разделе 5.5, где речь пойдет об автокодировщиках.

> И наконец, третий и самый общий класс задач обучения без учителя —
> задача *оценки плотности*: нам даны точки данных **{x1; ... ; xN}** и, возможно, какие-то
> априорные представления о том, откуда взялись эти точки, а хочется оценить
> распределение **p(x)**, из которого они получились. Это очень общая постановка задачи,
> к ней можно многое свести, и нейронные сети тоже отчасти ее и решают.

> Нередко в жизни возникает нечто среднее между обучением с учителем и
> обучением без учителя. Так обычно получается тогда, когда неразмеченные примеры
> найти очень легко, а размеченные получить сложно. Например, во все том же
> примере с синтаксическим разбором набрать сколько угодно неразмеченных текстов
> не представляет никакой сложности, а вот вручную нарисовать даже одно дерево
> нелегко. Или, скажем, задумайтесь о том, что такое «размеченные данные» для
> распознавания речи. Просто установить соответствие между звуковым файлом с
> речью и текстом, особенно если тексты достаточно длинные, здесь может быть
> недостаточно. По-настоящему размеченные данные для распознавания — это звуковые
> файлы, в которых вручную отмечены (или хотя бы проверены) границы
> *каждой фонемы*, каждого звука человеческой речи. Это адский труд, обычно делегируемый
> младшим научным сотрудникам, но даже целая армия лингвистов-фонологов
> будет двигаться достаточно медленно. А неразмеченных звуков живой человеческой
> речи вы можете записать сколько угодно, просто включив диктофон на запись.
> Такую ситуацию иногда называют *обучением с частичным привлечением учителя*,
> или полуконтролируемым обучением (английский термин *semi-supervised learning*
> устоялся куда больше), и с ней мы тоже не раз столкнемся в этой книге.

> В главе 9 мы с вами встретимся с другой постановкой задачи — *обучением с подкреплением*
> (reinforcement learning), когда агент, находясь в некоей среде,
> производит те или иные действия и получает за это награды. Цель агента — получить как
> можно большую награду с течением времени, а для этого неплохо бы понять, какие
> действия в конечном счете ведут к успеху. Так можно обучиться играть в разные
> игры или, например, сделать отличный протокол для A/B-тестирования.

#### 1.4. Особенности человеческого мозга

> ... важное замечание про работу человеческого
> мозга: мы распознаем лицо человека за пару сотен миллисекунд. А частота
> импульсов в аксонах бывает от 10 Гц в неактивном состоянии до примерно 200 Гц во
> время самой сильной активации. Это значит, что связи между отдельными нейронами
> активируются минимум за десятки миллисекунд, и в полном цикле распознавания
> человеческого лица не может быть последовательной цепочки активаций длиннее,
> чем буквально несколько нейронов; скорее всего, меньше десятка!

> То есть мозг, с одной стороны, содержит огромное число нейронов и еще больше
> связей между ними, но с другой — устроен очень плоско по сравнению с обычным
> процессором. Процессор в компьютере исполняет длинные последовательные
> цепочки команд, обрабатывая их в синхронном режиме, а у мозга цепочки короткие,
> зато работает он асинхронно (стохастически) и с очень высокой степенью
> параллелизации: нейроны активируются сразу во многих местах мозга, когда начинают
> распознавать лицо и делать много других увлекательных вещей. Можно сказать,
> что с этой точки зрения мозг больше похож на видеокарту, чем на процессор ...

> Основная идея искусственных нейронных сетей — собрать сеть из простых
> нейронов, активирующихся или не активирующихся в зависимости от
> снабженных поддающимися тренировке весами, — действительно позаимствована у
> природы. Однако дальше путь развития искусственного интеллекта отошел от
> природы; настоящие нейроны устроены значительно сложнее, чем те модели, которые
> мы будем обсуждать в этой книге.

#### 1.5. Пределы нейробиологии: что мы на самом деле знаем?

> В своей недавней яркой работе «Может ли нейробиолог понять микропроцессор?» [263] 
> (Кстати говоря, название статьи – отсылка к уже ставшей классической статье Юрия
> Лазебника «Может ли биолог починить радиоприемник» [299], перепечатанной в начале 2000-х годов тремя
> разными журналами, включая Cell. В этой статье Лазебник пытается проанализировать сломанный
> радиоприемник «Океан» методами биологических наук со столь же неутешительными выводами.)
> Эрик Джонас и Конрад Кординг пытаются проследить, насколько
> бы получилось у методов современной нейробиологии проанализировать
> какой-нибудь очень простой «мозг» на примере процессора MOS 6502. Такие процессоры
> устанавливались в Apple I и Atari VCS

> Для анализа Джонас и Кординг использовали классические методы, которыми
> нейробиология изучает настоящий человеческий мозг. Например, они специально
> симулировали повреждения отдельных транзисторов, чтобы узнать, за что они
> «отвечают». В рамках MOS 6502 они могли позволить себе попробовать повредить
> буквально каждый транзистор по отдельности. И действительно, они сумели
> выделить отдельные подмножества транзисторов, которые были необходимы для
> запуска каждой из трех рассмотренных игр (Space Invaders, Donkey Kong и Pitfall);
> без такого транзистора одна игра не запускалась, а две другие работали
> нормально. Можно было бы предположить, что эти транзисторы являются ключевыми для
> именно этого конкретного «поведения»... Вот только на самом деле не было ничего
> подобного: большинство этих транзисторов на самом деле реализовывали простые
> функции, например сложение, и чисто случайно именно эта часть реализации
> оказывалась нужна лишь в одной игре. Конечно, если бы исследователи заранее знали,
> что эта часть «мозга» реализует сложение, и затем начали повреждать отдельные
> транзисторы, смысла в этом было бы больше. Но ведь в реальной нейробиологии
> мы тоже совершенно не умеем изолировать настолько простые, базовые функции.

> Да и сами глубокие сети, о которых мы будем говорить в этой книге, не
> перестают удивлять исследователей. Хотя в данном случае люди сами
> запрограммировали этот «мыслительный аппарат» и твердо знают, как работает каждый
> конкретный «нейрон» сети, есть работы, посвященные буквально изучению свойств
> глубоких нейронных сетей, как будто они были бы природным объектом, данным
> нам для изучения (собственно, они таковым и являются, с той лишь разницей, что
> все же представляют собой математическую абстракцию). Например, в широко
> известной работе [249] рассказано, как можно обмануть сети, распознающие
> изображения (даже такие простые изображения, как черно-белые рукописные цифры),
> с помощью микроизменений, не влияющих на человеческое восприятие. Обученная
> глубокая сеть здесь выступает как «черный ящик», интересные свойства которого
> мы пытаемся понять.

#### 1.6. Блеск и нищета современных нейронных сетей

> Первым значительным индустриальным приложением современных глубоких
> нейронных сетей, которое подтвердило, что революция глубокого обучения
> действительно начинает необратимо менять ландшафт машинного обучения, да и
> вообще мира вокруг нас, стали успехи в распознавании речи. Структура
> полноценного распознавателя речи выглядит так:

1) сначала звуковой сигнал преобразуется в признаки специального вида;
2) затем эти признаки превращаются в гипотезы, которые предлагают варианты
конкретных фонем для каждого окна в звуковом сигнале;
3) потом гипотезы о фонемах объединяются в гипотезы относительно
произнесенных слов, и в выборе между ними уже участвует не только обработка
самого звука, но и языковые модели.

> До глубоких сетей первые два шага этого процесса выглядели так: сначала
> звуковой сигнал превращался в так называемые MFCC-признаки 
> (расшифровывается как mel-frequency cepstral coefficients, то есть от сигнала нужно сначала
> взять преобразование Фурье, получив спектр, затем перейти к логарифмам амплитуд частот на шкале
> мелов, а потом взять от этих логарифмов обратное преобразование Фурье, получив кепстр (это слово
> получилось обращением первых четырех букв слова «спектр») [379].), фонемы из них
>распознавали с помощью скрытых марковских моделей [119], а языковые модели
> представляли собой обычно сглаженные распределения n-грамм, то есть модели,
> которые оценивают вероятность следующего слова при условии нескольких
> предыдущих [281]

> Глубокие сети начали с того, что заменили собой распознаватель, основанный
> на скрытых марковских моделях. Более того, быстро стало ясно, что MFCC-признаки
> тоже можно улучшить путем обучения: нет, сети пока что не начинают работу
>непосредственно с необработанного звукового сигнала, но представления,
> подающиеся на вход современным системам распознавания, гораздо более «сырые»,
> чем MFCC [260].

> Вслед за речью пришло время обработки изображений. Точнее, она была всегда:
> с 1980-х годов в группе Яна ЛеКуна изображения обрабатывали именно нейронными
> сетями. К тому же времени относится и первый взлет сверточных нейронных сетей,
> специальной архитектуры, которая отлично подходит для обработки именно
>таких входов, как картинки; сверточным сетям мы посвятим главу 5 [18, 205]. В
> целом, это редкий пример области, в которой нейронные сети никогда полностью не
> пропадали из виду. Однако после начала революции глубокого обучения прогресс
> в обработке изображений тоже резко ускорился. В 2009–2010 годах глубокие
> сверточные сети выиграли ряд соревнований по распознаванию символов [396] и даже
> распознаванию видео с камер слежения [259, 430]. Кроме того, в 2009 году
> появились первые реализации нейронных сетей на графических процессорах [434], что
> дало огромный импульс всем связанным со сверточными сетями исследованиям
> (видеокарты для сверточных сетей особенно полезны).

>Достижения глубоких нейронных сетей в обучении с подкреплением тоже трудно
> переоценить. Глубокие сети оказались как нельзя кстати, потому что дают
> универсальный «черный ящик», способный приблизить функцию «оценки позиции».
> Даже первые значительные успехи обучения с подкреплением в конце 1980-х уже
> были основаны на нейронных сетях. Последний громкий успех глубоких сетей
> в обучении с подкреплением — созданная DeepMind программа AlphaGo, которая
> сумела обыграть одного из лучших игроков мира в го — одну из последних
> классических игр с полной информацией, которые считались крайне сложными для
>компьютера

> люди очень хороши в том, что называется переносом обучения
> (transfer learning): мы можем быстро построить модель нового объекта или
> процесса, порождая правильные абстракции из очень, очень маленького числа
> обучающих примеров. Известно, что дети с трех до 14–15 лет изучают в среднем 8–9
> новых слов каждый день1. Довольно очевидно, что они не могут получить большое
> число разных контекстов для каждого нового слова и должны обучаться по считанным
> единицам тренировочных примеров. Сейчас начинают проводиться исследования о том,
> как перенести такое обучение (его обычно называют обучением по одному примеру,
> one-shot learning) в нейронные сети и модели машинного обучения в целом.

### Глава 2. Предварительные сведения, *или Курс молодого бойца*

> Во второй главе мы дадим краткий обзор предварительных сведений, требующихся
> для того, чтобы дальше перейти непосредственно к нейронным сетям.
>А именно, мы рассмотрим:

- основы теории вероятностей, теорему Байеса и вероятностный подход к машинному обучению;
- функции ошибки в машинном обучении и регуляризацию;
- различие между регрессией и классификацией, функции ошибки для классификации;
- главный метод оптимизации в нейронных сетях — градиентный спуск;
- конструкцию графа вычислений и алгоритмы дифференцирования на нем;
- и наконец, практическое введение в библиотеку TensorFlow, которую мы будем использовать для примеров в этой книге.

#### 2.1. Теорема Байеса

> Как мы уже
> видели в предыдущей главе, машинное обучение — это наука о том, как на
> основании данных делать выводы, откуда эти данные взялись, и предсказания, какие
> данные встретятся нам в будущем. Важно, что делать точные выводы невозможно:
> процессы, приводящие к порождению данных, слишком сложны даже в самых
> простых случаях.
> Простейший пример: если знать заранее все параметры броска монеты: начальную скорость,
> состояние окружающего воздуха, распределение массы и т. п., то теоретически вполне возможно
> рассчитать ее полет и достаточно уверенно предсказать, чем она выпадет. Но это настолько сложно, что мы не
> задумываясь используем монету как честный генератор случайных чисел.

> бывают *дискретные* случайные величины с конечным или счетным набором
> исходов; каждому из своих исходов они присваивают неотрицательную
> вероятность, и вероятности исходов в сумме дают единицу; классический и
> фактически единственный пример здесь — бросание кубика;

> бывают *одномерные непрерывные* случайные величины, у которых набор
> исходов представляет собой вещественную прямую **R**; тогда вероятности
> отдельных исходов превращаются в функцию распределения **F(a) = p(x < a)**
> и ее производную (в этом месте может быть много сложностей, но практически
> всегда в наших примерах функция **F** будет непрерывно дифференцируемой),
> плотность распределения **p(x) = dF/dx** теперь не сумма, а интеграл неотрицательной
> функции плотности должен быть равен единице

> *Совместная вероятность* — это вероятность одновременного наступления
> двух событий, **p(x, y)**. Грубо говоря, если есть два кубика, на каждом из
> которых могут выпасть числа от 1 до 6, то мы можем рассмотреть новую
> случайную величину «два кубика», у которой будут возможные исходы (1; 1), (1; 2), (1; 3)
> и так далее до (6; 6), всего 6 на 6 = 36 исходов. Две случайные величины называются
> *независимыми*, если:
> **p(x, y) = p(x) p(y)**

> Чтобы получить обратно из совместной вероятности вероятность того или иного
> исхода одной из случайных величин, нужно просуммировать по другой.
> Этот процесс иногда называется умным словом *маргинализация*: если
> рассмотреть ее в случае непрерывных случайных величин, получится, что мы фактически
> проецируем двумерное распределение, поверхность в трехмерном пространстве, на
> одну из осей, получая функцию от одной переменной.

> *Условная вероятность* — вероятность наступления одного события, если
> известно, что произошло другое, **p(x | y)**, ее обычно определяют формально так:
> **p(x | y) = p(x, y) / p(y)**

> Аналогично обычной независимости можно теперь определить *условную независимость*:
> **x** и **y** условно независимы при условии **z**, если
> **p(x, y | z) = p(x | z) p(y | z)**

> Чтобы проиллюстрировать все эти базовые определения, в качестве примера
> рассмотрим известный «парадокс» Монти Холла (в кавычках потому, что никакого
> парадокса здесь на самом деле нет). Представьте себе, что на игровом шоу
> усатый ведущий предлагает вам выбрать из трех абсолютно одинаковых шкатулок:
> в одной из них лежат деньги, в двух других ничего нет. Вы принимаете решение
> (пока что у вас нет никаких оснований предпочесть одну из шкатулок), после чего
> ведущий открывает одну из двух оставшихся и показывает, что в ней пусто.
> Очевидно, он всегда может так сделать: даже если вы выбрали пустую шкатулку, из
> двух оставшихся все равно одна пустая найдется. И теперь наступает момент
> истины: ведущий предлагает вам изменить свое решение и взять вместо выбранной
> ту шкатулку, которая осталась закрытой. Выгодно ли вам это делать?

> Многие предполагают, что нет никакой разницы. И действительно, рассмотрим
> события наличия денег в трех шкатулках, обозначив их через **x1**, **x2** и **x3**
> соответственно. Изначально их вероятности были равны: **p(x1) = p(x2) = p(x3) = 1/3**
> Если бы ведущий просто открыл одну из шкатулок (для определенности пусть это
> будет **x3**) до вашего выбора, то две другие шкатулки остались бы равновероятными:
> **p(x1 | x3 = 0) = p(x2 | x3 = 0) = 1/2**

> Но структура эксперимента устроена сложнее! Ведущий открывает не
> случайную пустую шкатулку, а одну из двух не выбранных игроком. Поэтому события
> «какую из двух пустых шкатулок открыть» и «лежат ли деньги в выбранной вами
> шкатулке» становятся зависимыми. Давайте без потери общности предположим,
> что вы изначально выбрали первую шкатулку, а выбранную ведущим шкатулку
> обозначим через **y**. Будем предполагать, что если деньги действительно лежат в
> выбранной вами первой шкатулке (**x1 = 1**), то ведущий выбирает одну из двух пустых
> равновероятно. Тогда совместные вероятности разложатся так:

**p(x1 = 1, y = 2) = 1/6, p(x2 = 1, y = 2) = 0, p(x3 = 1, y = 2) = 1/3**,

**p(x1 = 1, y = 3) = 1/6, p(x2 = 1, y = 3) = 1/3, p(x3 = 1, y = 3) = 0**,

> и это совсем не похоже на определение независимых величин, правда?

> По определению условной вероятности:

**p(x; y) = p(x | y) p(y) = p(y | x) p(x)**;

**p(y | x) = p(x | y) p(y) / p(x)**

> ...формула, которая у нас получилась, — это, конечно, всего
> лишь очень простое формальное следствие определения условной вероятности. Но
> вместе с тем это самая главная формула всего машинного обучения, — формула,
> или теорема Байеса. В ней действительно нет ничего сложного, однако именно
> выводы, основанные на теореме Байеса, становятся ключевыми и в машинном обучении,
> и просто в наших житейских рассуждениях. Дело в том, что формула Байеса
> позволяет переоценивать наши априорные представления о мире (в формуле выше
> это **p(y)**) на основе частичной информации (данных), которую мы получили в виде
> наблюдений (в формуле выше это **p(x | y)**), в качестве вывода получая новое
> состояние наших представлений **p(y | x)**.

> Предположим, что некий тест на какую-нибудь страшную болезнь имеет вероятность
> успеха *95%*; иначе говоря, *5%* — это вероятность как ошибки первого рода
> (ложного срабатывания, false positive), так и ошибки второго рода (пропуска больного
> человека, false negative). Предположим также, что болезнь очень распространена
> и имеется у *1%* респондентов. Отложим на время то, что респонденты могут
> быть разного возраста и профессий — будем предполагать, что больные люди в нашем
> эксперименте выбираются из популяции случайно и равномерно.

> Пусть теперь некий человек (все так же случайно и равномерно выбранный
> из популяции) получил позитивный результат теста, то есть тест говорит, что
> страшная болезнь у человека присутствует. С какой вероятностью он действительно
> болен? Попробуйте, прежде чем подсчитать или прочитать ответ, оценить свою
> интуицию: как бы вы оценили вероятность болезни, если бы получили позитивный
> результат от такого теста?

> ...всего ***16%***! Почему так мало? На самом деле, если вдуматься в условия задачи,
> ответ *16%* покажется достаточно ясным: грубо говоря, из *100%* у вас всего
> *1%* на то, чтобы оказаться действительно больным, и *5%* на то, что тест ошибся
> и выдал неверный положительный результат. Значит, условная вероятность быть
> больным при условии положительного теста примерно равна **1 / (1+5) = 1/6**. Это
> грубый подсчет, совсем не учитывающий ошибку теста в другую сторону, но порядок
> величины получается верный.

> Дело в том, что в классической теории вероятностей, происходящей из физики,
> вероятность обычно понимается как предел отношения количества определенного
> результата эксперимента к общему количеству экспериментов. Стандартный пример
> здесь — это бросание монеты: если бросить честную монету тысячу раз, число
> выпавших решек будет довольно близко к пятистам, хотя вряд ли точно равно 500
> (вот, кстати, небольшое упражнение на понимание: какова будет точная вероятность
> получить ровно 500 решек из 1000 бросаний честной монеты?).

- расчёт вероятности такого события производится по
[формуле Бернули](https://ru.wikipedia.org/wiki/Формула_Бернулли)

> Задача о медицинском тестировании, которую мы сейчас решали, является
> примером так называемой обратной задачи теории вероятностей. Прямые задачи
> теории вероятностей возникают, когда дано описание некоего вероятностного процесса
> или модели, а найти требуется вероятность того или иного события, то есть
> фактически по модели предсказать поведение. Например: в урне лежат десять
> шаров, из них три черных; какова вероятность выбрать черный шар? Или: в урне
> лежат десять шаров с номерами от 1 до 10; какова вероятность того, что номера трех
> последовательно выбранных шаров дадут в сумме 12?

> А обратные задачи, напротив, просят по известному поведению некоего стохастического
> объекта построить вероятностную модель. Например: перед нами две
> урны, в каждой по десять шаров, но известно, что в одной три черных, а в другой —
> шесть. Кто-то взял из одной из урн шар, и шар оказался черным. Насколько вероятно,
> что шар брали из первой урны?

> Такую же обратную задачу можно решить и в парадоксе Монти Холла. Раз величины
> **y** (какую шкатулку откроет ведущий) и **x1** (будут ли деньги в выбранной шкатулке)
> оказались зависимыми, можно предположить, что тот факт, что мы
> узнали **y**, может помочь нам по-новому оценить вероятность **x1**.

**p(x1 = 1 | y = 2) = 1/3**

**p(x1 = 0 | y = 2) = 2/3**

> Это значит, что в парадоксе Монти Холла действительно выгодно изменить
> свое решение. Ответ, опять же, вполне интуитивен, если посмотреть под правильным
> углом: вы фактически выбираете между одной шкатулкой и обеими оставшимися,
> потому что из тех двух выбор уже сделали за вас.

> Классическая, «фриквентистская» (то есть основанная на частотах) теория вероятностей
> рассуждает о том, как оперировать вероятностями повторяющихся событий,
> когда можно устремить число экспериментов к бесконечности; например,
> высказывание «монета выпадает решкой с вероятностью 1/2» означает, что при
> большом числе подбрасываний решек получится примерно половина, причем чем
> больше экспериментов, тем ближе отношение должно быть к 1/2.
> Но в жизни часто возникает необходимость оценить вероятность событий,
> к которым такие рассуждения совершенно неприменимы.

> Все это события, о вероятностях которых вполне хочется рассуждать, а иногда
> и нужно рассуждать для решения практических задач; например, шансы сборной
> России пока еще, кажется, интересуют букмекеров. Но о «стремящемся к бесконечности
> числе экспериментов» здесь говорить бессмысленно — эксперимент ровно
> один. Можно, конечно, всласть натеоретизироваться о «возможных мирах» и квантовой
> неопределенности, но эти рассуждения все равно не помогут нам провести
> много экспериментов и узнать их результаты.

> В таких случаях вероятности уже выступают как степени доверия (degrees of
> belief). В такой интерпретации можно рассматривать теорию вероятностей как
> некое расширение обычной пропозициональной логики. В классической логике
> речь идет о строгих законах вывода, верных всегда, но можно рассмотреть и правила
> вывода для операций, работающих с вероятностями различных высказываний.

> Например, если я считаю, что с вероятностью **0,2** завтра пойдет дождь, а с
> вероятностью **0,7** мне нужно будет завтра идти на работу, и делаю предположение о том,
> что события эти независимы, значит, с вероятностью **0,7 x 0,2 = 0,14** я завтра пойду
> на работу под дождем. Число **0,14** здесь представляет собой или попытку объективной
> оценки правдоподобия того события, что я завтра пойду под дождем на работу
> (обратите внимание, что событие по-прежнему уникально, повторяющихся экспериментов
> с погодой не бывает), — такой взгляд называют объективистским, —
> или, в субъективистском подходе, просто представляет собой мое личное мнение
> об этом будущем событии. Заметьте, что в обратных задачах вероятности сразу стали
> байесовскими: в задаче о медицинском тестировании мне интересно, насколько
> правдоподобно то, что болен лично я со своим уникальным положительным результатом
> теста (в данном примере, впрочем, задачу легко переформулировать через
> частоты, но это не всегда так).

> Все это и составляет байесовский подход к вероятностям.
> ...
> Ну а сама идея применения теоремы Байеса для пересчета вероятностей между
> априорными и апостериорными гипотезами, как ни странно, действительно восходит
> к работе Томаса Байеса «Очерки к решению проблемы доктрины шансов» (An
> Essay towards solving a Problem in the Doctrine of Chances), вышедшей уже после
> смерти автора, в 1763 году [36].

> В работе Байеса впервые вводилось достаточно строгое определение понятия
> условной вероятности и решалась типичная задача байесовского вывода, обратная
> задача теории вероятностей: предположим, что некто пронаблюдал, как из урны
> с лотерейными билетами достали десять пустых билетов и один выигрышный.
> Какова будет вероятность того, что отношение между пустыми и выигрышными
> билетами в урне находится между **9:1** и **11:1**? Байес доказал, что для десяти билетов
> она будет невелика, около ***7,7%***, а затем установил эти вероятности для большего
> числа наблюдений (вплоть до 1000 выигрышей из 10 000 билетов, где эта вероятность
> достигает **97%**), а также вывел общую формулу.

> Оказывается, теорема Байеса — это основной, центральный инструмент машинного
> обучения, на ней держатся буквально все рассуждения этой книги и многие другие.
> Чтобы это увидеть, давайте запишем все ту же теорему Байеса в немного иных обозначениях:
> **p(teta | D) = p(teta) p(D | teta) / p(D)**
> И введем общепринятую в машинном обучении терминологию:

- **p(teta)** — априорная вероятность (prior probability);
- **p(D | teta)** — правдоподобие (likelihood);
- **p(teta | D)** — апостериорная вероятность (posterior probability);
- **p(D)** — вероятность данных (evidence).

> А что все это значит в реальности, алгоритмически? Что мы будем делать, решая
> задачи машинного обучения? Будем решать задачи оптимизации. У нас получилось,
> что обучение практически любой модели машинного обучения сводится
> к задаче оптимизации либо апостериорного распределения
> ...
> либо на худой конец просто правдоподобия

> Подведем краткий итог, который нужно держать в голове при чтении этой книги,
> да и любого другого текста о машинном обучении:

- математическая модель в машинном обучении обычно представляет собой
задание распределения вероятностей на данных и параметрах **p(teta, D)**;
- иногда совместное распределение параметров и данных **p(teta, D)** моделируют
напрямую, но чаще — в виде произведения правдоподобия **p(D | teta)** и 
априорного распределения **p(teta)**;
- апостериорное распределение **p(teta | D)** по теореме Байеса можно получить
как (нормированное) произведение **p(teta) p(D | teta)**;
- задача машинного обучения обычно состоит в том, чтобы найти и/или максимизировать
распределение **p(teta | D)**, — важно понять, какие параметры лучше
всего подходят к имеющимся данным и нашим априорным представлениям,
а затем, при необходимости, делать новые предсказания из предсказательного
распределения **p(x | D)**;
- в реальности же все это обычно превращается в задачу оптимизации, обычно
оптимизации логарифма **log p(D | teta) + log p(teta)**, который состоит из собственно
логарифма правдоподобия модели и регуляризаторов.

>О возникающих здесь задачах оптимизации мы сейчас вкратце и поговорим.

#### 2.2. Функции ошибки и регуляризация

> Сейчас мы очень кратко введем основной метод оптимизации — *метод градиентного спуска*

> На нем будут основываться почти все остальные
> методы оптимизации, которые мы будем рассматривать. Суть его работы легче
> всего проиллюстрировать, если представить себе трехмерную поверхность функции
> от двух аргументов; интуитивно эта поверхность является значением функции
> ошибки от весов модели, которые мы обучаем. Теперь представим, что текущее
> значение параметров — это небольшой шарик, находящийся на данной поверхности.
> Наша задача — найти минимум функции ошибки, и эта задача довольно точно
> соответствует тому, что шарик хотел бы сделать под действием силы тяжести, —
> скатиться в самую глубокую яму.

> ... если говорить формально — в направлении,
> прямо противоположном градиенту к поверхности ...

> Иначе говоря, градиент — это то направление, в котором функция быстрее всего
> возрастает. А значит, направление, в котором она быстрее всего убывает, — это
> и есть направление, обратное градиенту.

> Давайте даже на будущее запишем это в виде псевдокода:

**u = - learning_rate * grad**

**theta += u**

> Первый — как именно подсчитать градиент в каждой точке для нейронной сети
> со множеством весов, сложно связанных друг с другом; пока будем предполагать,
> что делать это мы умеем и градиент известен на каждом шаге. Второй — как
> выбирать скорость обучения и нужно ли менять ее со временем

> Начнем с самого простого случая: классической задачи линейной регрессии. Мы
> будем строить линейную модель имеющихся данных.

> Таким образом, по вектору входов
> **x = (x1; : : : ;xp)** мы будем предсказывать выход y так:

**y(x) = x w**

> Как найти оптимальные параметры **w** по тренировочным данным **D**?
> Для этого логично выбрать какую-нибудь функцию ошибки. Часто используют так
> называемый метод наименьших квадратов, в котором минимизируют сумму квадратов
> отклонений предсказанных значений от истинных

> В данном конкретном случае, кстати, задачу минимизации **RSS(w)** можно решить
> точно — функция ошибки оказывается выпуклой, более того, квадратичной,
> и чтобы найти ее минимум, достаточно решить систему линейных уравнений.

> Однако обратите внимание, как вероятностный (пока что даже не байесовский)
> подход к линейной регрессии сразу же вынудил нас явно выписать все сделанные
> предположения! Теперь мы знаем, что сумма квадратов отклонений соответствует
> нормально распределенному шуму с нулевым средним — и если вдруг увидим, что
> ошибки у нас очевидно имеют другую природу (например, если бывают погрешности
> только «в плюс», а «в минус» не бывают), сможем догадаться поискать другую
> функцию ошибки.

> Следующий пункт
> плана — регуляризация. Когда параметров у модели становится очень много, она
> начинает слишком хорошо «облизывать» точки из тренировочного набора данных,
> а ее предсказательная способность от этого страдает.

> Обнаружилась проблема — чем больше степень многочлена, тем, конечно, точнее
> им можно приблизить данные, но в какой-то момент результаты приближения
> перестанут иметь отношение к действительности. Это классический пример
> *оверфиттинга*. Как нам с ним справиться? Давайте сначала подойдем с несколько
> неожиданной стороны. В некоторый момент коэффициенты многочлена начинают
> очень сильно расти.

> Коэффициенты получаются гораздо больше, чем мы могли бы предположить
> о нашей кривой априори (опять это слово...). Давайте попробуем найти способ
> с этим справиться. Сначала будем действовать прямолинейно и простодушно: добавим
> размер коэффициентов в функцию ошибки в качестве дополнительного слагаемого;
> такие слагаемые называются *регуляризаторами*.

> Позволим себе небольшое лирическое отступление. Само слово «регуляризация»
> здесь выглядит немного странно; это потому, что оно имеет скорее исторический
> смысл. Дело в том, что изначально речь шла о решении линейных уравнений
> вида **Ax = b** и исследовании динамических систем с матрицей **A**. Если решения
> нет, то есть матрица **A** вырожденная, оказывается, что все равно можно пытаться
> сделать с этим уравнением что-то разумное. Для этого нужно заменить матрицу **A**
> на близкую к ней матрицу, как показано выше. При таком преобразовании
> вырожденная матрица обязательно снова станет невырожденной, «регулярной» —
> отсюда и «регуляризация». Кстати, с линейных уравнений на более общий случай
> операторов регуляризацию обобщил А. Н. Тихонов, и в его честь одну из форм
> регуляризации, к которой относится и гребневая регрессия, до сих пор называют
> «регуляризацией по Тихонову» (Tikhonov regularization).

> Но пока регуляризация, хоть и работает, выглядит как в высшей степени *ad hoc*
> решение: мы взяли и по своей воле добавили лишний член в функцию ошибки. Как
> интерпретировать это новое слагаемое с вероятностных позиций?

> Интуитивно смысл в следующем: мы решили, что в рассматриваемой модели
> «маловероятно», что у многочлена получатся большие коэффициенты. Иначе говоря,
> добавляя регуляризатор, мы пытались формализовать тот факт, что небольшие,
> короткие векторы коэффициентов более вероятны, чем длинные.

> Чтобы объяснить эту формализацию, давайте посмотрим на регрессию с совсем
> байесовской стороны. До сих пор в нашем анализе линейной регрессии участвовало
> только правдоподобие. Теперь введем какое-нибудь априорное распределение,
> которое будет выражать наши априорные представления о том, какими должны
> быть веса регрессии **w**.

> Обратите внимание: у нас получилось, что при нормальном априорном распределении
> **p(w)** и правдоподобии нормально распределенного шума **p(t | X)**
> с фиксированной дисперсией логарифм апостериорного распределения **p(w | t)**
> тоже получился квадратичной функцией от **w**, то есть **p(w | t)** — это тоже нормальное
> распределение! Это значит, что при фиксированной дисперсии и неизвестном
> среднем нормальное распределение является *самосопряженным*, то есть сопряженным
> самому себе.

> В этом разделе
> мы говорили исключительно о регрессии, то есть о предсказании вещественного
> числа, функцией ошибки для которого часто вполне разумно взять просто сумму
> квадратов отклонений. Но не менее важны для нас будут и задачи классификации
> — а там с функцией ошибки дело обстоит немного хитрее.

#### 2.3. Расстояние Кульбака — Лейблера и перекрестная энтропия

> ... большинство задач машинного обучения с учителем
> можно условно разделить на задачи регрессии, где целевая функция непрерывна,
> и задачи классификации, где целевая функция представляет собой выбор из
> нескольких классов; их может быть много (как, например, в распознавании лиц,
> которое так хорошо делает Facebook), но они все-таки дискретны, и каждому из
> них должна соответствовать целая область в пространстве параметров.

> Мы только что выяснили, что для задачи регрессии хорошей функцией ошибки
> является сумма квадратов отклонений предсказанных ответов от правильных. Эта
> функция соответствует нормально распределенному шуму, что для непрерывных
> величин более чем логично.

> Такая метрика, которую называют точностью (accuracy) классификации, действительно
> часто представляет собой нашу конечную цель. Но вот беда: функция
> ошибки, которая просто подсчитывает число верных ответов, — это кусочно-постоянная
> функция. Она всегда локально постоянна, и маленькие изменения в классификаторе
> практически никогда не приведут к изменению ответа на каких-то тестовых примерах.

> Из теории информации в информатику пришло понятие *относительной энтропии*, или
> *расстояния Кульбака - Лейблера* (Kullback - Leibler divergence, KL
> divergence, relative entropy), названного так в честь Соломона Кульбака и Ричарда
> Лейблера. Расстояние Кульбака - Лейблера является по своей сути мерой разницы
> между двумя вероятностными распределениями **P** и **Q**. Как правило, считается,
> что распределение **P** — это «истинное» распределение, а **Q** — его приближение,
> и тогда расстояние Кульбака - Лейблера служит оценкой качества приближения.

> Из формул сразу видно, что расстояние Кульбака - Лейблера на самом деле
> вовсе не является расстоянием! В частности, оно несимметрично: часто бывает,
> что **KL (P∥Q) ̸= KL (Q∥P)**. Но для нас важнее другое свойство: расстояние Кульбака -
> Лейблера всегда неотрицательно, **KL (P∥Q) >= 0**, и оно равно нулю только
> тогда, когда распределения **p** и **q** совпадают почти всюду.

> Минимизировать будем не совсем расстояние Кульбака - Лейблера, а так называемую
> *перекрестную энтропию* (cross-entropy)

> Так будет удобнее для минимизации, а с расстоянием Кульбака - Лейблера 
> перекрестная энтропия **H(p, q)** связана самым прямым образом

**KL (P∥Q) = H(p) + H(p, q)**

> С этой функцией ошибки тесно связана и классическая линейная модель классификации,
> которая называется логистической регрессией. Именно она обычно размещается на
> последнем уровне даже самых глубоких нейронных сетей: когда все
> признаки выделены, нужно в итоге на них все-таки сделать какой-то классификатор,
> и логистическая регрессия здесь подходит лучше всего.

**sigma(a) = 1 / (1 + e^(-a))**

> Функция **sigma(a)** называется логистическим сигмоидом; это одна из классических
> функций активации для отдельных перцептронов

> Обратите внимание: у нас опять получилась та же форма функции ошибки, что
> и выше! Это стандартная функция ошибки для задач классификации. Она так-же
> тривиальным образом обобщается на несколько классов: вместо логистического
> сигмоида будем теперь рассматривать так называемую *softmax*-функцию (сглаженный
> максимум, то есть на самом деле просто нормализованную экспоненту).

> Давайте закодируем поступающие на вход правильные ответы в виде векторов длины **K**,
> в каждом из которых все компоненты равны нулю, кроме правильного класса, где
> стоит единица (это называется *one-hot* кодированием, и мы с ним не раз еще встретимся).

> Однако здесь мы
> не будем вдаваться в подробности обучения логистической регрессии. Все равно
> она для нас больше интересна как часть большой нейронной сети, верхний слой,
> так что и обучать ее мы будем вместе со всей остальной сетью с помощью одного
> из универсальных алгоритмов обучения. В наше время практически все эти алгоритмы
> представляют собой по сути модификации градиентного спуска

#### 2.4. Градиентный спуск: основы

> Итак, мы выяснили, что основной нашей задачей и в машинном обучении вообще,
> и в обучении нейронных сетей в частности является уменьшение некоторой заданной
> функции ошибки. Теперь дело за малым — осталось понять, как же именно это
> сделать.

> По сути это означает, что мы должны научиться решать задачу оптимизации:
> по заданной функции найти аргументы, в которых эта функция максимизируется
> или минимизируется (это, понятное дело, одно и то же, ведь перед функцией
> всегда можно просто поставить минус).

> Хоть функция и действительно сложная, мы
> скоро увидим, что можем не только вычислить ее значение, но и взять от нее производную
> — а значит, для нас открыты все пути, связанные с градиентным спуском

> формально говоря, применение градиентного спуска возможно только на дифференцируемых
> функциях, где производная будет известна в любой точке. Но в жизни,
> конечно, иногда попадаются и недифференцируемые функции, а иногда — функции
> с тривиальными производными. Например, в обучении с подкреплением известно
> только текущее значение функции ценности. А в *обучении ранжированию*,
> когда мы хотим получить на выходе некое упорядочивание объектов по какому-то
> критерию, например в выдаче поисковика или рекомендательной системы, оказывается,
> что градиент функции ошибки почти всюду равен нулю!

> Поэтому, когда градиентный спуск неприменим, решение состоит в том, чтобы
> все-таки именно его и применить. В зависимости от того, в чем
> состоит сложность, можно воспользоваться такими приемами:

1) если производная есть, но вычислить ее не получается, а можно считать только
значения функций в разных точках, то производную можно подсчитать
приближенно ... а если вы когда-нибудь слушали курс
под названием «Методы вычислений» или подобный, вам могут быть знакомы
и чуть более сложные, но и более точные формулы конечных разностей;
2) если производной совсем нет и не предвидится, то приближать, скорее всего,
придется саму функцию, которую мы пытаемся оптимизировать; можно
попробовать выбрать достаточно «хорошо себя ведущую» функцию, которая,
тем не менее, похожа на ту, что надо оптимизировать;
3) если производная тривиальна, как в случае обучения ранжированию, то подлежащую
оптимизации функцию тоже приходится модифицировать; при
обучении ранжированию, например, мы будем оптимизировать не саму
функцию качества, зависящую только от расположения элементов в списке,
а некую специальную гладкую функцию ошибки, которая будет тем больше,
чем «более ошибочно» окажется вычисление релевантности;

> Конечно, от дискретной алгоритмической процедуры градиентного спуска
> точного попадания в ноль мы не дождемся, но в тот момент, когда изменения
> станут совсем маленькими, мы можем смело останавливать
>алгоритм градиентного спуска.

> Единственный (пока что!) параметр градиентного спуска — это его скорость
> обучения; она регулирует размер шага, который мы делаем в направлении
> склона-градиента.
> ... могут возникнуть две противоположные друг другу проблемы (рис. 2.6):

- если шаги будут слишком маленькими, то обучение будет слишком долгим,
и повышается вероятность застрять в небольшом неудачном локальном минимуме
по дороге (см. рис. 2.6, а);
- а если слишком большими, можно бесконечно прыгать через искомый минимум
 взад-вперед, но так и не прийти в самую нижнюю точку (см. рис. 2.6, б).

> Для того чтобы сделать один-единственный шаг градиентного спуска, нужно,
> получается, пробежаться по всему тренировочному множеству! А оно может быть,
> и так часто бывает в реальных задачах, гигантским. Неужели получается, что
> градиентный спуск на практике не работает?

> На самом деле, увы, действительно не работает. Работает не простой, а
> *стохастический* градиентный спуск, в котором ошибка подсчитывается и веса
> подправляются не после прохода по всему тренировочному множеству,
> а после каждого примера

> Основное преимущество стохастического градиентного спуска состоит в том,
> что ошибка на каждом шаге считается быстро, веса меняются сразу же, что очень
> сильно ускоряет обучение.

> Однако стохастический градиентный спуск — тоже не предел мечтаний: обновлять
> веса модели после каждого тренировочного примера зачастую выходит
> чересчур накладно. Поэтому на практике обычно используется нечто среднее:
> стохастический градиентный спуск по *мини-батчам* (mini-batch), небольшим
> подмножествам тренировочного набора.

#### 2.5. Граф вычислений и дифференцирование на нем

> Оказывается, что если у нас получится представить
> сложную функцию как композицию более простых, то мы сможем и эффективно
> вычислить ее производную по любой переменной, что и требуется для градиентного
> спуска. Самое удобное представление в виде композиции — это представление
> в виде *графа вычислений*. Граф вычислений — это граф, узлами которого являются
> функции (обычно достаточно простые, взятые из заранее фиксированного набора),
> а ребра связывают функции со своими аргументами.

> Если переиспользовать результаты можно и в итоге может получиться любой
> направленный ациклический граф, это схемная сложность, в которой вычисление
> функции представляется булевой схемой. А если нельзя и граф должен быть
> деревом, то получается формульная сложность. Она соответствует линейному размеру
> формулы, которой можно записать функцию — внутри формулы ведь не получится
> переобозначить большой кусок новой переменной.

> В нейронных сетях в качестве базисных, элементарных функций графа вычислений
> обычно используют функции, из которых получаются нейроны.

> Итак, мы поняли, что многие математические функции, даже с очень сложным
> поведением, можно представить в виде графа вычислений, где в узлах стоят
> элементарные функции, из которых, как из кирпичиков, получается сложная композиция,
> которую мы и хотим подсчитать. Собственно, с помощью такого графа даже
> с не слишком богатым набором элементарных функций можно приблизить любую
> функцию сколь угодно точно

> Под «лучше всего» здесь, как правило,
> имеется в виду оптимизация некоторой функции ошибки. Обычно она состоит из
> собственно ошибки на обучающей выборке (функции правдоподобия) и регуляризаторов
> (априорного распределения), но сейчас нам достаточно просто считать, что
> есть некая довольно сложная функция, которая дана нам свыше, и мы хотим ее
> минимизировать.

> ... градиентный спуск и граф вычислений буквально созданы друг для друга!
> чтобы вычислить производную композиции функций ..., достаточно уметь вычислять
> производные ее составляющих

> матрица частных производных называется матрицей Якоби1, а ее определитель —
> якобианом; они еще не раз нам встретятся. Теперь мы можем подсчитать
> производные и градиенты любой композиции функций, в том числе векторных,
> и для этого нужно только уметь вычислять производные каждой компоненты. Для
> графа все это де факто сводится к простой, но очень мощной идее: если мы знаем
> граф вычислений и знаем, как брать производную в каждом узле, этого достаточно,
> чтобы взять производную от всей сложной функции, которую задает граф!

> Таким образом, можно применять формулу дифференцирования композиции
> на графе либо от истоков к стокам, получая частные производные каждого узла
> по одной и той же переменной **x**, либо от стоков к истокам, получая
> частные производные стоков по всем промежуточным узлам **f**
> Конечно, на практике для машинного обучения нам нужен скорее второй вариант, чем
> первый: функция ошибки обычно одна, и нам требуются ее частные производные
> сразу по многим переменным, в особенности по всем весам, по которым мы хотим
> вести градиентный спуск.
 
> В общем виде алгоритм такой: предположим, что нам задан некоторый направленный
> ациклический граф вычислений **G = (V, E)**, вершинами которого являются функции
> **g** из **V** , причем часть вершин соответствует входным переменным
> **x1; ... ; xN** и не имеет входящих ребер, одна вершина не имеет исходящих ребер
> и соответствует функции **f** (весь граф вычисляет эту функцию), а ребра показывают
> зависимости между функциями, стоящими в узлах. Тогда мы уже знаем, как
> получить функцию **f**, стоящую в «последней» вершине графа: для этого достаточно
> двигаться по ребрам и вычислять каждую функцию в топологическом порядке.
> А чтобы узнать частные производные этой функции, достаточно двигаться в обратном
> направлении.

> Вот и все! Когда мы дойдем до истоков графа, до вершин **x1; ... ; xN**, мы получим
> частные производные **f**, то есть как раз вычислим градиент для **f**.

> Такой подход называют алгоритмом *обратного распространения* (backpropagation,
> backprop, bprop), потому что частные производные считаются в направлении,
> обратном ребрам графа вычислений. А алгоритм вычисления самой функции
> или производной по одной переменной, называют алгоритмом
> *прямого распространения* (forward propagation, fprop).

> ... важное замечание: обратите внимание, что за все то время, пока
> мы обсуждали графы вычислений, дифференциалы, градиенты и тому подобное,
> мы, собственно, ни разу всерьез не упомянули нейронные сети! И действительно,
> метод вычисления производных/градиентов по графу вычислений сам по себе совершенно
> никак не связан с нейронными сетями. Это полезно иметь в виду, особенно
> в делах практических, к которым мы перейдем уже в следующем разделе. Дело
> в том, что библиотеки Theano и TensorFlow, которые мы будем обсуждать ниже
> и на которых делается большая часть глубокого обучения, — это, вообще говоря,
> библиотеки для автоматического дифференцирования, а не для обучения нейронных
> сетей. Все, что они делают, — позволяют вам задать граф вычислений и чертовски
> эффективно, с распараллеливанием и переносом на видеокарты, вычисляют
> градиент по этому графу.

#### 2.6. И о практике: введение в TensorFlow и Keras

> В последние годы обучение нейронных сетей превратилось в высшей степени инженерную дисциплину.

> С каждым годом эти библиотеки становятся все удобнее, выходят новые версии существующих,
> а также абсолютно новые программные продукты. Есть и библиотеки общего назначения,
> которые могут создать любой граф вычислений, и специализированные надстройки,
> которые реализуют разные компоненты нейронных
> сетей: обычные слои, сверточные, рекуррентные, рекуррентные слои из LSTM или
> GRU, современные алгоритмы оптимизации... Все это обычно можно «пощупать
> руками», реализовать и обучить в домашних условиях, без долгих и мучительных
> процессов разработки.

> Среди библиотек общего назначения, которые способны строить граф вычислений
> и проводить автоматическое дифференцирование, долгое время бесспорным лидером
> была Theano, разработанная в университете Монреаля, в группе
> классика глубокого обучения Йошуа Бенджи (Yoshua Bengio) [528, 529]. Однако
> в ноябре 2015 года Google выпустила (с открытым исходным кодом) библиотеку
> TensorFlow [523], предназначенную для того же самого. TensorFlow стала вторым
> поколением библиотек глубокого обучения в Google; она была призвана заменить
> библиотеку DistBelief [295], которая послужила многим выдающимся результатам
> в истории глубокого обучения, но так и осталась проприетарной.

> Основная абстракция, которая потребуется нам для того, чтобы понять все,
> что происходит в коде таких библиотек, как TensorFlow или Theano, — это все
> тот же граф вычислений, который мы рассматривали в предыдущем разделе.
> Программа, использующая TensorFlow, обычно просто задает граф вычислений,
> а потом запускает процедуру вроде session.run, которая выполняет вышеописанные
> вычисления и получает собственно результаты.

> Однако, в отличие от многих других библиотек, TensorFlow, будучи
> детищем Google, умеет еще и «из коробки» распараллеливать обучение на распределенные
> кластеры компьютеров

> Основной объект, которым оперирует
> TensorFlow, — это, как можно понять буквально из названия, тензор,
> или многомерный массив чисел.

>> Читающие это математики наверняка пришли в ужас: как так, тензор — это же элемент
>> тензорного пространства, то есть по сути линейное преобразование между многомерными
>> линейными пространствами. Преобразования тоже образуют линейное пространство,
>> а числа — это всего лишь их координатные представления, они зависят от выбора базиса
>> и могут меняться при том, что сам тензор как геометрический объект останется неизменным.
>> Что ж, вынуждены математиков разочаровать: в TensorFlow тензор — это не настоящий тензор,
>> а именно многомерный массив, никакого мотивированного геометрией отношения эквивалентности
>> на них нет, просто слово красивое. Но раз уж это слово используется
>> насквозь во всей документации и даже в названии TensorFlow, избежать его нам не удастся.

> Переменная в TensorFlow — это некий буфер в памяти, который содержит тензоры.

> В TensorFlow реализован полный набор операций над тензорами из NumPy
> с поддержкой матричных вычислений над массивами разной формы и конвертирования
> между этими формами (broadcasting).

> Broadcasting применим для всех поэлементных операций над двумя тензорами
> и устроен следующим образом. Размерности двух тензоров последовательно
> сравниваются начиная с конца; при каждом
> сравнении необходимо выполнение одного из двух условий:

- либо размерности равны;
- либо одна из размерностей равна 1.

> При этом тензоры не обязаны иметь одинаковую размерность: недостающие измерения
> меньшего из тензоров будут интерпретированы как единичные. Размерность
> получаемого на выходе тензора, если все условия выполнены, вычисляется
> как максимум из соответствующих размерностей исходных тензоров. Впрочем, это
> звучит довольно сложно, так что рекомендуем внимательно проверять, как именно
> будет работать broadcasting в каждом конкретном нетривиальном случае.

> Давайте рассмотрим простейший пример модели, своего рода Hello World для
> TensorFlow — обучение линейной регрессии. Напомним, что линейная регрессия —
> это фактически один нейрон, который получает на вход вектор значений **x**,
> выдает число, и на данных **{(x1, y1);(x2, y2); ... ; (xN; yN)}** задача
> состоит в том, чтобы минимизировать сумму квадратов отклонений оценок
> нейрона ***yi*** от истинных значений **yi**

> Но хоть задача и очень простая, мы постараемся в этом примере продемонстрировать
> полный цикл типичной программы на TensorFlow, с градиентным спуском,
> обучением на мини-батчах, заглушками и блекджеком.

> Вот как это можно сделать. В коде, приведенном ниже, мы будет приближать
> линейной регрессией функцию вида **f = kx + b** для **k = 2** и **b = 1**;
> k и b будут параметрами, которые мы хотим обучить.

> Обратите внимание, что TensorFlow чрезвычайно чувствительна к формам
> (в смысле shapes, размерностям) тензоров. Например, чтобы работала функция
> ***tf.matmul***, нужно подать ей на вход обязательно матрицы, даже если это матрица
> размера 1 x 1, как у нас; а вот складывать результат можно и с обычным вектором
> длины 1 (да, это разные вещи!), форма которого задается как (1,).

> Естественно, точных значений **k = 2**, **b = 1** на выходе не получится:
> и градиентный спуск является всего лишь приближенным методом оптимизации, и, что
> в данном случае важнее, правильный ответ на задачу оптимизации не обязательно
> совпадает с «задуманными» нами значениями, ведь данные генерировались
> случайным образом.

> Вторая библиотека, которой мы часто будем пользоваться в этой книге, — это Keras [79].

>Keras реализует практически все то, о чем мы будем говорить
> в этой книге, в виде готовых примитивов: можно сказать «сделай мне сверточный
> слой с такими-то параметрами», и Keras сделает. Однако «за кулисами» происходит
> все то же самое, что в TensorFlow: строится граф вычислений, который Keras
> потом скормит другой библиотеке для вычисления градиентов и реализации
> оптимизационных алгоритмов.

> Конечно, в основном Keras хорош именно тем, что в нем много чего уже реализовано в готовом виде.

### Глава 3. Перцептрон, *или Эмбрион мудрого компьютера*

> В третьей главе мы подробно рассмотрим главную составляющую любой нейронной сети —
> перцептрон, а также то, как перцептроны соединяются в сети.
> В частности, мы поговорим:
• об истории искусственных нейронных сетей;
• об определении перцептрона и методах его обучения;
• о разных нелинейных функциях активации, от классических до современных;
• о том, похожа ли наша модель перцептрона на настоящие живые нейроны;
• о том, как соединять нейроны в сеть и почему это совсем не такое простое
дело, как могло бы показаться.
> А затем, обсудив все это, приведем живой пример сети, которая обучится распознавать
> рукописные цифры.

#### 3.1. Когда появились искусственные нейронные сети

> По видимому, первой работой, предлагающей математическую модель нейрона
> и конструкцию искусственной нейронной сети, была статья Уоррена Маккаллоха
> (Warren McCulloch) и Уолтера Питтса [356], опубликованная в 1943 году. Авторы
> отмечают, что из-за бинарной природы нейронной активности (нейрон либо
> «включен», либо «выключен», практически без промежуточных состояний) нейроны
> удобно описывать в терминах пропозициональной логики, а для нейронных
> сетей разрабатывают целый логический аппарат, формализующий ациклические
> графы. Сама конструкция искусственного нейрона, который у Маккаллоха и Питтса
> называется Threshold Logic Unit (TLU), или Linear Threshold Unit, получилась
> очень современной: линейная комбинация входов, которая затем поступает на вход
> нелинейности в виде «ступеньки», сравнивающей результат с некоторым порогом
> (threshold).

> Основная конкретная идея, перешедшая из работ Хебба в современное машинное
> обучение практически без изменений, — это так называемое правило Хебба,
> которое сам Дональд Хебб в первоисточнике формулировал так: «Когда аксон
> клетки A находится достаточно близко, чтобы возбудить клетку B, и многократно
> или постоянно участвует в том, чтобы ее активировать, в одной или обеих
> клетках происходит некий процесс роста или изменение метаболизма,
> в результате которого эффективность A как клетки, возбуждающей B, увеличивается».
> Проще говоря, если связь между двумя нейронами часто используется,
> она от этого упражняется и становится сильнее. Эта простая, но очень мощная
> идея не только мотивировала дальнейшие исследования, но и сама по себе легла
> в основу так называемого обучения по Хеббу (Hebbian learning),
> группы методов обучения без учителя, основанных на этом базовом правиле.

> Следующим прорывом в нейробиологии стали работы Хьюбела и Визеля [235,
> 236, 569], которые смогли достаточно подробно изучить активации нейронов в
> зрительной коре, что стало мотивацией для появления сверточных нейронных сетей
> и в некотором смысле глубокого обучения вообще.

> упомянем один подробный обзор современной истории нейронных сетей, работу
> Юргена Шмидхубера [475].

>> Юрген Шмидхубер (Jurgen Schmidhuber, р. 1963) — немецкий математик и информатик, один из
>> отцов-основателей современного машинного обучения, знаменитый своими достижениями в области
>> нейронных сетей, генетических алгоритмов, теории сложности и других областях. В частности,
>> лаборатория Шмидхубера разработала ряд конструкций современных рекуррентных нейронных сетей,
>> включая LSTM (long short-term memory). Одно любопытное направление исследований Шмидхубера —
>> теория красоты, основанная на понятии колмогоровской сложности, и минималистические произведения
>> искусства, создаваемые очень простыми алгоритмами (low complexity art) [473, 474].

> Несмотря на то что это очень
> плотный и довольно сухой текст, зачастую представляющий собой просто набор
> ссылок, он выгодно отличается от многих других обзоров и исторических очерков
> тем, что анализирует именно историю *идей*

#### 3.2. Как работает перцептрон

#### 3.3. Современные перцептроны: функции активации

#### 3.4. Как же обучаются настоящие нейроны

#### 3.5. Глубокие сети: в чем прелесть и в чем сложность?

#### 3.6. Пример: распознавание рукописных цифр на TensorFlow

## Часть II. Основные архитектуры

### Глава 4. Быстрее, глубже, сильнее, *или Об оврагах, долинах и трамплинах*

#### 4.1. Регуляризация в нейронных сетях

#### 4.2. Как инициализировать веса

#### 4.3. Нормализация по мини-батчам

#### 4.4. Метод моментов: Ньютон, Нестеров и Гессе

#### 4.5. Адаптивные варианты градиентного спуска

### Глава 5. Сверточные нейронные сети и автокодировщики, *или Не верь глазам своим*

#### 5.1. Зрительная кора головного мозга

#### 5.2. Свертки и сверточные сети

#### 5.3. Свертки для распознавания цифр

#### 5.4. Современные сверточные архитектуры

#### 5.5. Автокодировщики

#### 5.6. Пример: кодируем рукописные цифры

### Глава 6. Рекуррентные нейронные сети, *или Как правильно кусать себя за хвост*

#### 6.1. Мотивация: обработка последовательностей

#### 6.2. Распространение ошибки и архитектуры RNN

#### 6.3. LSTM

#### 6.4. GRU и другие варианты

#### 6.5. SCRN и другие: долгая память в обычных RNN

#### 6.6. Пример: порождаем текст символ за символом

## Часть III. Новые архитектуры и применения

### Глава 7. Как научить компьютер читать, *или Математик - Мужчина + Женщина =*

#### 7.1. Интеллектуальная обработка текстов

#### 7.2. Распределенные представления слов: word2vec

#### 7.3. Русскоязычный word2vec на практике

#### 7.4. GloVe: раскладываем матрицу правильно

#### 7.5. Вверх и вниз от представлений слов

#### 7.6. Рекурсивные нейронные сети и синтаксический разбор

### Глава 8. Современные архитектуры, *или Как в споре рождается истина*

#### 8.1. Модели с вниманием и encoder-decoder

#### 8.2. Порождающие модели и глубокое обучение

#### 8.3. Состязательные сети

#### 8.4. Практический пример и трюк с логистическим сигмоидом

#### 8.5. Архитектуры, основанные на GAN

### Глава 9. Глубокое обучение с подкреплением, *или Удивительное происшествие с чемпионом*

#### 9.1. Обучение с подкреплением

#### 9.2. Марковские процессы принятия решений

#### 9.3. От TDGammon к DQN

#### 9.4. Бамбуковая хлопушка

#### 9.5. Градиент по стратегиям и другие применения

### Глава 10. Нейробайесовские методы, *или Прошлое и будущее машинного обучения*

#### 10.1. Теорема Байеса и нейронные сети

#### 10.2. Алгоритм EM

#### 10.3. Вариационные приближения

#### 10.4. Вариационный автокодировщик

#### 10.5. Байесовские нейронные сети и дропаут

#### 10.6. Заключение: что не вошло в книгу и что будет дальше
